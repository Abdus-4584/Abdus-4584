{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "#pip install --upgrade google-cloud-storage\n",
    "#pip install --upgrade google-cloud-spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84990256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ibm_db_dbi as idb\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "import warnings\n",
    "import pyvalidata as pvd\n",
    "import hashlib\n",
    "#import time \n",
    "#import datetime\n",
    "#from datetime import datetime\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import ibm_db_sa\n",
    "import csv\n",
    "from google.cloud import storage\n",
    "from google.cloud import spanner\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import spanner\n",
    "from datetime import datetime, timedelta,date, timezone  # Import 'timezone' from 'datetime'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e47f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"kr-7738-supvis-t-3e70a14d235d_test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0174fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to udm-db (database 1) successful.\n",
      "Connection to storewalk-db (database 2) successful.\n"
     ]
    }
   ],
   "source": [
    "#projects/kr-7738-supvis-t/instances/nc-event-sink-instance/databases/storewalks-db\n",
    "\n",
    "# Your Cloud Spanner instance ID.\n",
    "instance_id = \"nc-event-sink-instance\"\n",
    "\n",
    "# Your Cloud Spanner database ID.\n",
    "database_id_1 = \"udm-db\"\n",
    "database_id_2 = \"storewalks-db\"\n",
    "\n",
    "# Instantiate a client.\n",
    "spanner_client = spanner.Client()\n",
    "\n",
    "# Get a Cloud Spanner instance by ID.\n",
    "instance = spanner_client.instance(instance_id)\n",
    "\n",
    "# Get a Cloud Spanner database by ID (for database_id_1).\n",
    "database_1 = instance.database(database_id_1)\n",
    "print(\"Connection to udm-db (database 1) successful.\")\n",
    "\n",
    "# Get a Cloud Spanner database by ID (for database_id_1).\n",
    "database_2 = instance.database(database_id_2)\n",
    "print(\"Connection to storewalk-db (database 2) successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b481e45",
   "metadata": {},
   "source": [
    "## Define Parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f0041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dates you want to query (in UTC)\n",
    "#Daily:\n",
    "#Source\n",
    "daily_date_utc = datetime(2023, 9, 5, tzinfo=timezone.utc)\n",
    "#Target\n",
    "daily_tgt_task_date = date(2023, 9, 5)\n",
    "\n",
    "#Weekly Sanitation:\n",
    "#Source-\n",
    "weekly_sntn_str_date_utc = datetime(2023, 8, 30, tzinfo=timezone.utc)\n",
    "weekly_sntn_end_date_utc = datetime(2023, 9, 5, tzinfo=timezone.utc)\n",
    "weekly_sntn_src_task_date = date(2023, 9, 4)\n",
    "#Target-\n",
    "weekly_sntn_tgt_task_date = date(2023, 9, 5)\n",
    "\n",
    "#Weekly Rotation:\n",
    "#Source-\n",
    "weekly_rtn_str_date_utc = datetime(2023, 8, 30, tzinfo=timezone.utc)\n",
    "weekly_rtn_end_date_utc = datetime(2023, 9, 5, tzinfo=timezone.utc)\n",
    "weekly_rtn_src_task_date = date(2023, 9, 4)\n",
    "#Target-\n",
    "weekly_rtn_tgt_task_date = date(2023, 9, 5)\n",
    "\n",
    "#3xWeekly:\n",
    "#Source-\n",
    "weekly3_str_date_utc = datetime(2023, 8, 11, tzinfo=timezone.utc)\n",
    "weekly3_end_date_utc = datetime(2023, 8, 11, tzinfo=timezone.utc)\n",
    "weekly3_src_task_date = date(2023, 8, 11)\n",
    "#Target-\n",
    "weekly3_tgt_task_date = date(2023, 8, 11)\n",
    "\n",
    "#Periodic:\n",
    "#Source-\n",
    "periodic_str_date_utc = datetime(2023, 8, 30, tzinfo=timezone.utc)\n",
    "periodic_end_date_utc = datetime(2023, 9, 5, tzinfo=timezone.utc)\n",
    "periodic_src_task_date = date(2023, 9, 4)\n",
    "#Target-\n",
    "periodic_tgt_task_date = date(2023, 9, 5)\n",
    "\n",
    "#Quaterly:\n",
    "#Source\n",
    "quaterly_str_date_utc = datetime(2023, 8, 11, tzinfo=timezone.utc)\n",
    "quaterly_end_date_utc = datetime(2023, 8, 11, tzinfo=timezone.utc)\n",
    "#Target\n",
    "quaterly_tgt_task_date = date(2023, 8, 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e267bd",
   "metadata": {},
   "source": [
    "## 1) Daily Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60a5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_1 = \"udm-db\"  & instance_id = \"nc-event-sink-instance\" (Source Query)\n",
    "#Source Data\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and strt_date = @daily_date_utc\n",
    "                AND task_classification LIKE 'Daily'\n",
    "                --AND div_str_id = '53100515'\n",
    "            UNION ALL\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and schd_strt_date = @daily_date_utc\n",
    "                AND task_classification LIKE 'Daily'\n",
    "                --AND div_str_id = '53100515'\n",
    "        ) Daily_S\n",
    "    \"\"\", params={'daily_date_utc': daily_date_utc}, \n",
    "         param_types={'daily_date_utc': spanner.param_types.TIMESTAMP}\n",
    "                )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_s_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae538d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 109079\n",
      "Number of columns: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>div_str_id_s</th>\n",
       "      <th>task_type_name</th>\n",
       "      <th>task_classification_s</th>\n",
       "      <th>task_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01100002</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Spot Clean Dairy Display Cases, Glass Doors &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70800789</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Spot Clean Dairy Display Cases, Glass Doors &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70800787</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Spot Clean Dairy Display Cases, Glass Doors &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70800570</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Spot Clean Dairy Display Cases, Glass Doors &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70800568</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Spot Clean Dairy Display Cases, Glass Doors &amp; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  div_str_id_s task_type_name task_classification_s  \\\n",
       "0     01100002     Sanitation                 Daily   \n",
       "1     70800789     Sanitation                 Daily   \n",
       "2     70800787     Sanitation                 Daily   \n",
       "3     70800570     Sanitation                 Daily   \n",
       "4     70800568     Sanitation                 Daily   \n",
       "\n",
       "                                           task_desc  \n",
       "0  Spot Clean Dairy Display Cases, Glass Doors & ...  \n",
       "1  Spot Clean Dairy Display Cases, Glass Doors & ...  \n",
       "2  Spot Clean Dairy Display Cases, Glass Doors & ...  \n",
       "3  Spot Clean Dairy Display Cases, Glass Doors & ...  \n",
       "4  Spot Clean Dairy Display Cases, Glass Doors & ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_rows, num_columns = result_s_df.shape\n",
    "#print(f\"Number of rows: {num_rows}\")\n",
    "#print(f\"Number of columns: {num_columns}\")\n",
    "#result_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1691e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_s_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_ds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2280ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_2 = \"storewalks-db\"  & instance_id = \"nc-event-sink-instance\"\n",
    "#Target Data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT\n",
    "            CONCAT(division_number, store_number) AS div_str_id,\n",
    "            task_type,\n",
    "            task_classification,\n",
    "            task_description\n",
    "        FROM\n",
    "            task\n",
    "        WHERE\n",
    "            task_type <> 'Green Rack'\n",
    "            AND task_classification LIKE 'Daily'\n",
    "            AND task_date = @daily_tgt_task_date\n",
    "            --AND CONCAT(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'daily_tgt_task_date': daily_tgt_task_date},\n",
    "       param_types={'daily_tgt_task_date': spanner.param_types.DATE})\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_t_df = pd.DataFrame(rows, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e03a1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_t_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_ds_t.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3b3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 108989\n",
      "Number of columns: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>div_str_id</th>\n",
       "      <th>task_type</th>\n",
       "      <th>task_classification</th>\n",
       "      <th>task_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53100515</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Wipe Down - Glass Doors on Sales Floor Display...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02900714</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Deep Cleaning - Restroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01600832</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Take Out Trash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>03400747</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Wash/Rinse/Sanitize - Sinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01600977</td>\n",
       "      <td>Sanitation</td>\n",
       "      <td>Daily</td>\n",
       "      <td>Empty Trash (Inside/Outside)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  div_str_id   task_type task_classification  \\\n",
       "0   53100515  Sanitation               Daily   \n",
       "1   02900714  Sanitation               Daily   \n",
       "2   01600832  Sanitation               Daily   \n",
       "3   03400747  Sanitation               Daily   \n",
       "4   01600977  Sanitation               Daily   \n",
       "\n",
       "                                    task_description  \n",
       "0  Wipe Down - Glass Doors on Sales Floor Display...  \n",
       "1                           Deep Cleaning - Restroom  \n",
       "2                                     Take Out Trash  \n",
       "3                        Wash/Rinse/Sanitize - Sinks  \n",
       "4                       Empty Trash (Inside/Outside)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_rows, num_columns = result_t_df.shape\n",
    "#print(f\"Number of rows: {num_rows}\")\n",
    "#print(f\"Number of columns: {num_columns}\")\n",
    "#result_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab13a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Validation Report Failed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "merged_df = pd.merge(result_s_df, result_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (merged_df['task_type_name'] == merged_df['task_type']) &\n",
    "    (merged_df['task_classification_s'] == merged_df['task_classification']) &\n",
    "    #(merged_df['sub_dpmt_name'] == merged_df['department_name']) &\n",
    "    (merged_df['task_desc'] == merged_df['task_description'])\n",
    "    #(merged_df['concatenated_s'] == merged_df['concatenated_t'])\n",
    ")\n",
    "merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (merged_df['div_str_id_s'].notna()) & (merged_df['div_str_id'].isna())\n",
    "merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (merged_df['div_str_id'].notna()) & (merged_df['div_str_id_s'].isna())\n",
    "merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from merged_df\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = merged_df[merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Daily Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Daily_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Daily Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5323372",
   "metadata": {},
   "source": [
    "## 2) Weekly Sanitation Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904990b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (strt_date) >= @weekly_sntn_str_date_utc\n",
    "and (strt_date) <= @weekly_sntn_end_date_utc\n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @weekly_sntn_str_date_utc\n",
    "and (schd_strt_date) <= @weekly_sntn_end_date_utc\n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\", params={'weekly_sntn_str_date_utc': weekly_sntn_str_date_utc,\n",
    "              'weekly_sntn_end_date_utc': weekly_sntn_end_date_utc}, \n",
    "         param_types={'weekly_sntn_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly_sntn_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0147126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' --and  concat(division_number,store_number) ='53100515'\n",
    "and task_classification like 'Weekly' \n",
    "and task_date= @weekly_sntn_src_task_date\n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'weekly_sntn_src_task_date': weekly_sntn_src_task_date},\n",
    "       param_types={'weekly_sntn_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da297580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_ws_s_df = pd.merge(result_ws_s1_df, result_ws_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_ws_s_df = result_ws_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_ws_s_df = result_ws_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf4dbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_ws_s_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_ws_s.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed2eab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date= @weekly_sntn_tgt_task_date\n",
    "and task_type='Sanitation'\n",
    "--and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'weekly_sntn_tgt_task_date': weekly_sntn_tgt_task_date},\n",
    "       param_types={'weekly_sntn_tgt_task_date': spanner.param_types.DATE})\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2065ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_ws_t_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_ws_t.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c14006c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly Sanitation Validation Report Failed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "ws_merged_df = pd.merge(result_ws_s_df, result_ws_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "ws_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (ws_merged_df['task_type_name'] == ws_merged_df['task_type']) &\n",
    "    (ws_merged_df['task_classification_s'] == ws_merged_df['task_classification']) &\n",
    "    #(ws_merged_df['sub_dpmt_name'] == ws_merged_df['department_name']) &\n",
    "    (ws_merged_df['task_desc'] == ws_merged_df['task_description'])\n",
    "    #(ws_merged_df['concatenated_s'] == ws_merged_df['concatenated_t'])\n",
    ")\n",
    "ws_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (ws_merged_df['div_str_id_s'].notna()) & (ws_merged_df['div_str_id'].isna())\n",
    "ws_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (ws_merged_df['div_str_id'].notna()) & (ws_merged_df['div_str_id_s'].isna())\n",
    "ws_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from ws_merged_df\n",
    "ws_merged_df = ws_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "ws_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = ws_merged_df[ws_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Weekly Sanitation Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Weekly_Sanitation_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Weekly Sanitation Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98eb6be",
   "metadata": {},
   "source": [
    "## 3) Weekly Rotation Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63d78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' \n",
    "and c.is_actv='Y' \n",
    "and (strt_date) >= @weekly_rtn_str_date_utc\n",
    "and (strt_date) <= @weekly_rtn_end_date_utc\n",
    "and task_classification  like 'Weekly' \n",
    "--and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' \n",
    "and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @weekly_rtn_str_date_utc\n",
    "and (schd_strt_date) <= @weekly_rtn_end_date_utc\n",
    "and task_classification  like 'Weekly' \n",
    "--and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\", params={'weekly_rtn_str_date_utc': weekly_rtn_str_date_utc,\n",
    "              'weekly_rtn_end_date_utc': weekly_rtn_end_date_utc}, \n",
    "         param_types={'weekly_rtn_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly_rtn_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a95d5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Rotation' --and concat(division_number,store_number) ='53100515' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date= @weekly_rtn_src_task_date\n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'weekly_rtn_src_task_date': weekly_rtn_src_task_date},\n",
    "       param_types={'weekly_rtn_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d66ba8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_wr_s_df = pd.merge(result_wr_s1_df, result_wr_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_wr_s_df = result_wr_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_wr_s_df = result_wr_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45b483c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_wr_s_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_wr_s.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d02da1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date= @weekly_rtn_tgt_task_date\n",
    "and task_type='Rotation'\n",
    "--and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'weekly_rtn_tgt_task_date': weekly_rtn_tgt_task_date},\n",
    "       param_types={'weekly_rtn_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99af5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_wr_t_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_wr_t.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec78530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly Rotation Validation Report Failed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "wr_merged_df = pd.merge(result_wr_s_df, result_wr_t_df, left_on=['div_str_id_s', 'task_desc']\n",
    "                        , right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "wr_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (wr_merged_df['task_type_name'] == wr_merged_df['task_type']) &\n",
    "    (wr_merged_df['task_classification_s'] == wr_merged_df['task_classification']) &\n",
    "    #(wr_merged_df['sub_dpmt_name'] == wr_merged_df['department_name']) &\n",
    "    (wr_merged_df['task_desc'] == wr_merged_df['task_description'])\n",
    "    #(wr_merged_df['concatenated_s'] == wr_merged_df['concatenated_t'])\n",
    ")\n",
    "wr_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (wr_merged_df['div_str_id_s'].notna()) & (wr_merged_df['div_str_id'].isna())\n",
    "wr_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (wr_merged_df['div_str_id'].notna()) & (wr_merged_df['div_str_id_s'].isna())\n",
    "wr_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from wr_merged_df\n",
    "wr_merged_df = wr_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "wr_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = wr_merged_df[wr_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Weekly Rotation Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Weekly_Rotation_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Weekly Rotation Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851517c",
   "metadata": {},
   "source": [
    "## 4) 3xWeekly Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dfa533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT div_str_id, a.task_desc, a.task_type, task_type_name, task_classification, sub_dpmt_name\n",
    "            FROM santn_rotn_Task_mstr a\n",
    "            JOIN sub_dpmt_mpng b USING (sub_dpmt_id)\n",
    "            JOIN str_dpmt_mpng c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "            LEFT JOIN rolng_task_addtnl_info d ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE a.is_actv = 'Y' \n",
    "              AND c.is_actv = 'Y' \n",
    "              AND (strt_date) >= @weekly3_str_date_utc\n",
    "              AND (strt_date) <= @weekly3_end_date_utc\n",
    "              --AND div_str_id = '53100515' \n",
    "              AND task_classification LIKE '3x Weekly'\n",
    "            UNION ALL\n",
    "            SELECT div_str_id, a.task_desc, a.task_type, task_type_name, task_classification, sub_dpmt_name\n",
    "            FROM santn_rotn_Task_mstr a\n",
    "            JOIN sub_dpmt_mpng b USING (sub_dpmt_id)\n",
    "            JOIN str_dpmt_mpng c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "            LEFT JOIN santn_rotn_task_schdl d ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE a.is_actv = 'Y' \n",
    "              AND c.is_actv = 'Y' \n",
    "              AND (schd_strt_date) >= @weekly3_str_date_utc \n",
    "              AND (schd_strt_date) <= @weekly3_end_date_utc \n",
    "              --AND div_str_id = '53100515' \n",
    "              AND task_classification LIKE '3x Weekly'\n",
    "        ) \n",
    " \"\"\", params={'weekly3_str_date_utc': weekly3_str_date_utc,\n",
    "              'weekly3_end_date_utc': weekly3_end_date_utc}, \n",
    "         param_types={'weekly3_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly3_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8f230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select distinct concat(division_number,store_number) AS div_str_id,\n",
    "from task where task_date= @weekly3_src_task_date --and  concat(division_number,store_number) ='53100515'\n",
    "        \"\"\", params={'weekly3_src_task_date': weekly3_src_task_date},\n",
    "       param_types={'weekly3_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a861f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_3w_s_df = pd.merge(result_3w_s1_df, result_3w_s2_df,left_on=['div_str_id'], right_on=['div_str_id'], how='inner')\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_3w_s_df = result_3w_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_3w_s_df = result_3w_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e11ec24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like '3x Weekly'\n",
    "and task_date= @weekly3_tgt_task_date\n",
    "--and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'weekly3_tgt_task_date': weekly3_tgt_task_date},\n",
    "       param_types={'weekly3_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fddbf389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3xWeekly Validation Report Passed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "w3_merged_df = pd.merge(result_3w_s_df, result_3w_t_df, left_on=['div_str_id_s', 'task_desc']\n",
    "                        , right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "w3_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (w3_merged_df['task_type_name'] == w3_merged_df['task_type']) &\n",
    "    (w3_merged_df['task_classification_s'] == w3_merged_df['task_classification']) &\n",
    "    #(w3_merged_df['sub_dpmt_name'] == w3_merged_df['department_name']) &\n",
    "    (w3_merged_df['task_desc'] == w3_merged_df['task_description'])\n",
    "    #(w3_merged_df['concatenated_s'] == w3_merged_df['concatenated_t'])\n",
    ")\n",
    "w3_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (w3_merged_df['div_str_id_s'].notna()) & (w3_merged_df['div_str_id'].isna())\n",
    "w3_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (w3_merged_df['div_str_id'].notna()) & (w3_merged_df['div_str_id_s'].isna())\n",
    "w3_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from w3_merged_df\n",
    "w3_merged_df = w3_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "w3_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = w3_merged_df[w3_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('3xWeekly Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'3xWeekly_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('3xWeekly Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a0ea4",
   "metadata": {},
   "source": [
    "## 5) Periodic Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93af2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (strt_date) >= @periodic_str_date_utc \n",
    "and (strt_date) <= @periodic_end_date_utc\n",
    "and task_classification  like 'Periodic' --and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @periodic_str_date_utc\n",
    "and (schd_strt_date) <= @periodic_end_date_utc \n",
    "and task_classification  like 'Periodic' --and div_str_id ='53100515'\n",
    ") \n",
    " \"\"\", params={'periodic_str_date_utc': periodic_str_date_utc,\n",
    "              'periodic_end_date_utc': periodic_end_date_utc}, \n",
    "         param_types={'periodic_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'periodic_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s1_df = pd.DataFrame(rows, columns = cols)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53de7ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Periodic' \n",
    "and task_date= @periodic_src_task_date --and concat(division_number, store_number) = '53100515'\n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'periodic_src_task_date': periodic_src_task_date},\n",
    "       param_types={'periodic_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5ccf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_p_s_df = pd.merge(result_p_s1_df, result_p_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_p_s_df = result_p_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_p_s_df = result_p_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd4f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_p_s_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_p_s.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f08ae5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Periodic'\n",
    "and task_date= @periodic_tgt_task_date\n",
    "--and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'periodic_tgt_task_date': periodic_tgt_task_date},\n",
    "       param_types={'periodic_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc1b3858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_p_t_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm_p_t.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd20328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Periodic Validation Report Failed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "p_merged_df = pd.merge(result_p_s_df, result_p_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "p_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (p_merged_df['task_type_name'] == p_merged_df['task_type']) &\n",
    "    (p_merged_df['task_classification_s'] == p_merged_df['task_classification']) &\n",
    "    #(p_merged_df['sub_dpmt_name'] == p_merged_df['department_name']) &\n",
    "    (p_merged_df['task_desc'] == p_merged_df['task_description'])\n",
    "    #(p_merged_df['concatenated_s'] == p_merged_df['concatenated_t'])\n",
    ")\n",
    "p_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (p_merged_df['div_str_id_s'].notna()) & (p_merged_df['div_str_id'].isna())\n",
    "p_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (p_merged_df['div_str_id'].notna()) & (p_merged_df['div_str_id_s'].isna())\n",
    "p_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from p_merged_df\n",
    "p_merged_df = p_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "p_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = p_merged_df[p_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Periodic Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Periodic_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Periodic Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb86ef4",
   "metadata": {},
   "source": [
    "## 6) Quaterly Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c408d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_1 = \"udm-db\"  & instance_id = \"nc-event-sink-instance\" (Source Query)\n",
    "\n",
    "#Source Data\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and (strt_date) >= @quaterly_str_date_utc\n",
    "                and (strt_date) <= @quaterly_end_date_utc \n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "                --AND div_str_id = '53100515'\n",
    "            UNION ALL\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and (schd_strt_date) >= @quaterly_str_date_utc\n",
    "                and (schd_strt_date) <= @quaterly_end_date_utc\n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "                --AND div_str_id = '53100515'\n",
    "        ) Daily_S\n",
    " \"\"\", params={'quaterly_str_date_utc': quaterly_str_date_utc,\n",
    "              'quaterly_end_date_utc': quaterly_end_date_utc}, \n",
    "         param_types={'quaterly_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'quaterly_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_s_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f34dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_2 = \"storewalks-db\"  & instance_id = \"nc-event-sink-instance\"\n",
    "#Target Data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT\n",
    "            CONCAT(division_number, store_number) AS div_str_id,\n",
    "            task_type,\n",
    "            task_classification,\n",
    "            task_description,\n",
    "        FROM\n",
    "            task\n",
    "        WHERE\n",
    "            task_type <> 'Green Rack'\n",
    "            AND task_classification LIKE 'Quarterly'\n",
    "            AND task_date = @quaterly_tgt_task_date\n",
    "            --AND CONCAT(division_number, store_number) = '53100515'\n",
    "        \"\"\", params={'quaterly_tgt_task_date': quaterly_tgt_task_date},\n",
    "       param_types={'quaterly_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_t_df = pd.DataFrame(rows, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e193d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quaterly Validation Report Passed\n"
     ]
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "q_merged_df = pd.merge(result_q_s_df, result_q_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "q_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (q_merged_df['task_type_name'] == q_merged_df['task_type']) &\n",
    "    (q_merged_df['task_classification_s'] == q_merged_df['task_classification']) &\n",
    "    #(q_merged_df['sub_dpmt_name'] == q_merged_df['department_name']) &\n",
    "    (q_merged_df['task_desc'] == q_merged_df['task_description'])\n",
    "    #(q_merged_df['concatenated_s'] == q_merged_df['concatenated_t'])\n",
    ")\n",
    "q_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (q_merged_df['div_str_id_s'].notna()) & (q_merged_df['div_str_id'].isna())\n",
    "q_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (q_merged_df['div_str_id'].notna()) & (q_merged_df['div_str_id_s'].isna())\n",
    "q_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from q_merged_df\n",
    "q_merged_df = q_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "q_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = q_merged_df[q_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Quaterly Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Quaterly_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Quaterly Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7add1",
   "metadata": {},
   "source": [
    "###  -------------- Counts  -------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84787d2",
   "metadata": {},
   "source": [
    "## 1) Daily Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f38b5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source Counts\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    " select sum(count), 'daily_s'  \n",
    "from    \n",
    " (\n",
    " (select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                AND (strt_date) = @daily_date_utc\n",
    "                --AND (strt_date) <= '2023-09-05'\n",
    "                AND task_classification LIKE 'Daily'\n",
    "group by div_str_id, task_type_name, task_classification,sub_dpmt_name)  \n",
    "union all\n",
    "(select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                AND (schd_strt_date) = @daily_date_utc\n",
    "                --AND DATE(schd_strt_date) <= '2023-09-05'\n",
    "                AND task_classification LIKE 'Daily'\n",
    "group by div_str_id, task_type_name, task_classification,sub_dpmt_name) \n",
    ")   \n",
    "    \"\"\", params={'daily_date_utc': daily_date_utc}, \n",
    "         param_types={'daily_date_utc': spanner.param_types.TIMESTAMP}\n",
    "                )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_s_c_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f2c7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Counts\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'daily_t' \n",
    "from \n",
    "(select count(*) as count1,division_number, store_number, task_type, task_classification,department_name, task_date, task_status \n",
    "from  task\n",
    "where task_type<>'Green Rack' and task_classification like 'Daily' and task_date= @daily_tgt_task_date\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status\n",
    ")\n",
    "        \"\"\", params={'daily_tgt_task_date': daily_tgt_task_date},\n",
    "       param_types={'daily_tgt_task_date': spanner.param_types.DATE})\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_t_c_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc779d",
   "metadata": {},
   "source": [
    "## 2) Weekly Sanitation Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d75aefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (strt_date) >= @weekly_sntn_str_date_utc\n",
    "and (strt_date) <= @weekly_sntn_end_date_utc\n",
    "and task_classification  like 'Weekly'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @weekly_sntn_str_date_utc\n",
    "and (schd_strt_date) <= @weekly_sntn_end_date_utc\n",
    "and task_classification  like 'Weekly'\n",
    ") \n",
    " \"\"\", params={'weekly_sntn_str_date_utc': weekly_sntn_str_date_utc,\n",
    "              'weekly_sntn_end_date_utc': weekly_sntn_end_date_utc}, \n",
    "         param_types={'weekly_sntn_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly_sntn_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s1_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c3626fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date= @weekly_sntn_src_task_date\n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'weekly_sntn_src_task_date': weekly_sntn_src_task_date},\n",
    "       param_types={'weekly_sntn_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f98b544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_ws_c_df = pd.merge(result_ws_s1_c_df, result_ws_s2_c_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_ws_c_df = result_ws_c_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_ws_c_df = result_ws_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_ws_c_count_df = result_ws_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count = result_ws_c_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label_df = pd.DataFrame({'count': [total_count], 'label': ['weekly_s']})\n",
    "\n",
    "#result_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "397092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'weekly_t' \n",
    "from \n",
    "(select count(*) as count1, division_number,store_number,task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date= @weekly_sntn_tgt_task_date\n",
    "and task_type='Sanitation'\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status,task_description\n",
    ")\n",
    "        \"\"\", params={'weekly_sntn_tgt_task_date': weekly_sntn_tgt_task_date},\n",
    "       param_types={'weekly_sntn_tgt_task_date': spanner.param_types.DATE})\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_t_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da2b5b7",
   "metadata": {},
   "source": [
    "## 3) Weekly Rotation Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a481d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (strt_date) >= @weekly_rtn_str_date_utc\n",
    "and (strt_date) <= @weekly_rtn_end_date_utc \n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @weekly_rtn_str_date_utc\n",
    "and (schd_strt_date) <= @weekly_rtn_end_date_utc\n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    ") \n",
    " \"\"\", params={'weekly_rtn_str_date_utc': weekly_rtn_str_date_utc,\n",
    "              'weekly_rtn_end_date_utc': weekly_rtn_end_date_utc}, \n",
    "         param_types={'weekly_rtn_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly_rtn_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s1_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bf5c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Rotation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date= @weekly_rtn_src_task_date\n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'weekly_rtn_src_task_date': weekly_rtn_src_task_date},\n",
    "       param_types={'weekly_rtn_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "343652c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4976</td>\n",
       "      <td>weekly_r_s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count       label\n",
       "0   4976  weekly_r_s"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_wr_c_df = pd.merge(result_wr_s1_c_df, result_wr_s2_c_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_wr_c_df = result_wr_c_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_wr_c_df = result_wr_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_wr_c_count_df = result_wr_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count2 = result_wr_c_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label2_df = pd.DataFrame({'count': [total_count2], 'label': ['weekly_r_s']})\n",
    "\n",
    "#result_label2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "818ff6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'weekly_r_t' \n",
    "from \n",
    "(select count(*) as count1, division_number,store_number,task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date= @weekly_rtn_tgt_task_date\n",
    "and task_type='Rotation'\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status,task_description\n",
    ")\n",
    "\n",
    "        \"\"\", params={'weekly_rtn_tgt_task_date': weekly_rtn_tgt_task_date},\n",
    "       param_types={'weekly_rtn_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_t_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a3aab",
   "metadata": {},
   "source": [
    "## 4) 3xWeekly Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cfc105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "  select  div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y'               \n",
    "AND (strt_date) >= @weekly3_str_date_utc\n",
    "AND (strt_date) <= @weekly3_end_date_utc\n",
    "AND task_classification like '3x Weekly'\n",
    "union all\n",
    "(select  div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "AND (schd_strt_date) >= @weekly3_str_date_utc \n",
    "AND (schd_strt_date) <= @weekly3_end_date_utc \n",
    "and task_classification like '3x Weekly'\n",
    ")\n",
    " \"\"\", params={'weekly3_str_date_utc': weekly3_str_date_utc,\n",
    "              'weekly3_end_date_utc': weekly3_end_date_utc}, \n",
    "         param_types={'weekly3_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'weekly3_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s1_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04981514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select distinct concat(division_number,store_number) AS div_str_id,\n",
    "from task where task_date= @weekly3_src_task_date \n",
    "        \"\"\", params={'weekly3_src_task_date': weekly3_src_task_date},\n",
    "       param_types={'weekly3_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0995dbd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2484</td>\n",
       "      <td>3xweekly_s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count       label\n",
       "0   2484  3xweekly_s"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_3w_s_c_df = pd.merge(result_3w_s1_c_df, result_3w_s2_c_df,left_on=['div_str_id'], right_on=['div_str_id'], how='inner')\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_3w_s_c_df = result_3w_s_c_df[['div_str_id', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_3w_s_c_df = result_3w_s_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_3w_c_count_df = result_3w_s_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count3 = result_3w_c_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label3_df = pd.DataFrame({'count': [total_count3], 'label': ['3xweekly_s']})\n",
    "\n",
    "#result_label3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3002fa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1),'3xweekly_t' from (\n",
    "select count(*) as count1 , concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like '3x Weekly'\n",
    "and task_date= @weekly3_tgt_task_date\n",
    "group by division_number, store_number, task_type, task_classification,department_name \n",
    ")\n",
    "        \"\"\", params={'weekly3_tgt_task_date': weekly3_tgt_task_date},\n",
    "       param_types={'weekly3_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_t_c_df = pd.DataFrame(rows, columns = cols)    \n",
    "#result_3w_t_c_df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d79f16",
   "metadata": {},
   "source": [
    "## 5) Periodic Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fbdd939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (strt_date) >= @periodic_str_date_utc \n",
    "and (strt_date) <= @periodic_end_date_utc\n",
    "and task_classification  like 'Periodic'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and (schd_strt_date) >= @periodic_str_date_utc\n",
    "and (schd_strt_date) <= @periodic_end_date_utc \n",
    "and task_classification  like 'Periodic'\n",
    ") \n",
    "\n",
    " \"\"\", params={'periodic_str_date_utc': periodic_str_date_utc,\n",
    "              'periodic_end_date_utc': periodic_end_date_utc}, \n",
    "         param_types={'periodic_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'periodic_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s1_c_df = pd.DataFrame(rows, columns = cols)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6952d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Periodic' \n",
    "and task_date= @periodic_src_task_date \n",
    "and task_status='To Do'\n",
    "        \"\"\", params={'periodic_src_task_date': periodic_src_task_date},\n",
    "       param_types={'periodic_src_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "31da344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_p_s_c_df = pd.merge(result_p_s1_c_df, result_p_s2_c_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_p_s_c_df = result_p_s_c_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_p_s_c_df = result_p_s_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_p_count_df = result_p_s_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count4 = result_p_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label4_df = pd.DataFrame({'count': [total_count4], 'label': ['Periodic_s']})\n",
    "\n",
    "#result_label4_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a7750a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1) , 'Periodic'\n",
    "from (\n",
    "select count(*) as count1,concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_status\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Periodic'\n",
    "and task_date= @periodic_tgt_task_date\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status \n",
    ")\n",
    "        \"\"\", params={'periodic_tgt_task_date': periodic_tgt_task_date},\n",
    "       param_types={'periodic_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_t_c_df = pd.DataFrame(rows, columns = cols)    \n",
    "#result_p_t_c_df.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf482fe8",
   "metadata": {},
   "source": [
    "## 6) Quaterly Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed2e160f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Quaterly_s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   \n",
       "0  None  Quaterly_s"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#database_id_1 = \"udm-db\"  & instance_id = \"nc-event-sink-instance\" (Source Query)\n",
    "\n",
    "#Source Data\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    " select sum(count), 'Quaterly_s'  \n",
    "from    \n",
    " (\n",
    " (select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and (strt_date) >= @quaterly_str_date_utc\n",
    "                and (strt_date) <= @quaterly_end_date_utc \n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "            group by div_str_id, task_type_name, task_classification,sub_dpmt_name)  \n",
    "            UNION ALL\n",
    "(select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and (schd_strt_date) >= @quaterly_str_date_utc\n",
    "                and (schd_strt_date) <= @quaterly_end_date_utc\n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "group by div_str_id, task_type_name, task_classification,sub_dpmt_name) \n",
    ")\n",
    " \"\"\", params={'quaterly_str_date_utc': quaterly_str_date_utc,\n",
    "              'quaterly_end_date_utc': quaterly_end_date_utc}, \n",
    "         param_types={'quaterly_str_date_utc': spanner.param_types.TIMESTAMP,\n",
    "                     'quaterly_end_date_utc': spanner.param_types.TIMESTAMP\n",
    "                     }\n",
    "                    )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_s_c_df = pd.DataFrame(rows, columns = cols)\n",
    "#result_q_s_c_df.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f09f43a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>Quaterly_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   \n",
       "0  None  Quaterly_t"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#database_id_2 = \"storewalks-db\"  & instance_id = \"nc-event-sink-instance\"\n",
    "#Target Data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'Quaterly_t' \n",
    "from \n",
    "(select count(*) as count1,division_number, store_number, task_type, task_classification,department_name, task_date, task_status \n",
    "from  task\n",
    "where task_type<>'Green Rack' and task_classification like 'Quarterly' AND task_date = @quaterly_tgt_task_date\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status\n",
    ")\n",
    "        \"\"\", params={'quaterly_tgt_task_date': quaterly_tgt_task_date},\n",
    "       param_types={'quaterly_tgt_task_date': spanner.param_types.DATE}\n",
    "                                 )\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_t_c_df = pd.DataFrame(rows, columns=cols)\n",
    "#result_q_t_c_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b13447",
   "metadata": {},
   "source": [
    "## Writing All counts to Report ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c7f195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts Report generated\n"
     ]
    }
   ],
   "source": [
    "# Get the current timestamp\n",
    "timestamp_c = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the file path\n",
    "file_path_c = f\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports\\\\Count_{timestamp_c}.csv\"\n",
    "\n",
    "# Write the DataFrames to the CSV file\n",
    "result_s_c_df.to_csv(file_path_c, index=False) #daily S\n",
    "result_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False)  #daily T\n",
    "result_label_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly_s S\n",
    "result_ws_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly_s T\n",
    "result_label2_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly_r S\n",
    "result_wr_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly_r T\n",
    "result_label3_df.to_csv(file_path_c, mode='a', header=False, index=False) #3weekly S\n",
    "result_3w_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #3weekly TT\n",
    "result_label4_df.to_csv(file_path_c, mode='a', header=False, index=False) #Periodic S\n",
    "result_p_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #Periodic T\n",
    "result_q_s_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #Quaterly S\n",
    "result_q_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #Quaterly T\n",
    "\n",
    "print(\"Counts Report generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0738e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0bd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a32e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec365245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac904ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22abb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf87bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee8c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29448730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccee83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd471f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d872c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401098f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc8b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a43d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b481485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counts_Report_Date \n",
    " Daily count Source - Done\n",
    " Daily count Target - Done\n",
    " Weekly Sanitation count Source - Done\n",
    " Weekly Sanitation count Target -Done\n",
    " Weekly Rotation count Source - Done \n",
    " Weekly Rotation count Target - Done\n",
    " 3xWeekly count Source - Done\n",
    " 3xWeekly count Target - Done\n",
    " Periodic count Source - Done\n",
    " Periodic count Target - Done\n",
    " Quaterly source - Done\n",
    " Quaterly target - Done\n",
    " \n",
    " ------------------------\n",
    "Daily_Validation_Report_Date - Done\n",
    "Weekly_Sanitation_Validation_Report_Date - Done\n",
    "Weekly_Rotation_Validation_Report_Date - Done\n",
    "3xWeekly_ReportValidation_Report_Date - DOne\n",
    "Periodic_Validation_Report_Date - Done\n",
    "Quaterly_Validation_Report_Date - Done\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
