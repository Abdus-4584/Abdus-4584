{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade google-api-python-client\n",
    "#pip install --upgrade google-cloud-storage\n",
    "#pip install --upgrade google-cloud-spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84990256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ibm_db_dbi as idb\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "import warnings\n",
    "import pyvalidata as pvd\n",
    "import hashlib\n",
    "#import time \n",
    "#import datetime\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import ibm_db_sa\n",
    "import csv\n",
    "from google.cloud import storage\n",
    "from google.cloud import spanner\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import spanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e47f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"kr-7738-supvis-t-test-app-deployer-sa.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0174fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to udm-db (database 1) successful.\n",
      "Connection to storewalks-db (database 2) successful.\n"
     ]
    }
   ],
   "source": [
    "#projects/kr-7738-supvis-t/instances/nc-event-sink-instance/databases/storewalks-db\n",
    "\n",
    "# Your Cloud Spanner instance ID.\n",
    "instance_id = \"nc-event-sink-instance\"\n",
    "\n",
    "# Your Cloud Spanner database ID.\n",
    "database_id_1 = \"udm-db\"\n",
    "database_id_2 = \"storewalks-db\"\n",
    "\n",
    "# Instantiate a client.\n",
    "spanner_client = spanner.Client()\n",
    "\n",
    "# Get a Cloud Spanner instance by ID.\n",
    "instance = spanner_client.instance(instance_id)\n",
    "\n",
    "# Get a Cloud Spanner database by ID (for database_id_1).\n",
    "database_1 = instance.database(database_id_1)\n",
    "print(\"Connection to udm-db (database 1) successful.\")\n",
    "\n",
    "# Get a Cloud Spanner database by ID (for database_id_1).\n",
    "database_2 = instance.database(database_id_2)\n",
    "print(\"Connection to storewalks-db (database 2) successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e267bd",
   "metadata": {},
   "source": [
    "## 1) Daily Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_1 = \"udm-db\"  & instance_id = \"nc-event-sink-instance\" (Source Query)\n",
    "\n",
    "#Source Data\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and DATE(strt_date) >='2023-09-05' \n",
    "                and DATE(strt_date) <='2023-09-05' \n",
    "                AND task_classification LIKE 'Daily'\n",
    "                AND div_str_id = '53100515'\n",
    "            UNION ALL\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and DATE(schd_strt_date) >='2023-09-05' \n",
    "                and DATE(schd_strt_date) <='2023-09-05'\n",
    "                AND task_classification LIKE 'Daily'\n",
    "                AND div_str_id = '53100515'\n",
    "        ) Daily_S\n",
    "    \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_s_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fcf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_s_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_2 = \"storewalks-db\"  & instance_id = \"nc-event-sink-instance\"\n",
    "#Target Data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT\n",
    "            CONCAT(division_number, store_number) AS div_str_id,\n",
    "            task_type,\n",
    "            task_classification,\n",
    "            task_description,\n",
    "        FROM\n",
    "            task\n",
    "        WHERE\n",
    "            task_type <> 'Green Rack'\n",
    "            AND task_classification LIKE 'Daily'\n",
    "            AND task_date = '2023-09-05'\n",
    "            AND CONCAT(division_number, store_number) = '53100515'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_t_df = pd.DataFrame(rows, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b930b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_t_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab13a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "merged_df = pd.merge(result_s_df, result_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (merged_df['task_type_name'] == merged_df['task_type']) &\n",
    "    (merged_df['task_classification_s'] == merged_df['task_classification']) &\n",
    "    #(merged_df['sub_dpmt_name'] == merged_df['department_name']) &\n",
    "    (merged_df['task_desc'] == merged_df['task_description'])\n",
    "    #(merged_df['concatenated_s'] == merged_df['concatenated_t'])\n",
    ")\n",
    "merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (merged_df['div_str_id_s'].notna()) & (merged_df['div_str_id'].isna())\n",
    "merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (merged_df['div_str_id'].notna()) & (merged_df['div_str_id_s'].isna())\n",
    "merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from merged_df\n",
    "merged_df = merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = merged_df[merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Daily Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Daily_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Daily Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5323372",
   "metadata": {},
   "source": [
    "## 2) Weekly Sanitation Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904990b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(strt_date) >='2023-08-30' \n",
    "and DATE(strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(schd_strt_date) >='2023-08-30' \n",
    "and DATE(schd_strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date='2023-09-04' \n",
    "and task_status='To Do'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e04bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ws_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da297580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_ws_s_df = pd.merge(result_ws_s1_df, result_ws_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_ws_s_df = result_ws_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_ws_s_df = result_ws_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d967637",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_ws_s_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_ws_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2eab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date='2023-09-05'\n",
    "and task_type='Sanitation'\n",
    "and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca6b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_ws_t_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_ws_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14006c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "ws_merged_df = pd.merge(result_ws_s_df, result_ws_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "ws_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (ws_merged_df['task_type_name'] == ws_merged_df['task_type']) &\n",
    "    (ws_merged_df['task_classification_s'] == ws_merged_df['task_classification']) &\n",
    "    #(ws_merged_df['sub_dpmt_name'] == ws_merged_df['department_name']) &\n",
    "    (ws_merged_df['task_desc'] == ws_merged_df['task_description'])\n",
    "    #(ws_merged_df['concatenated_s'] == ws_merged_df['concatenated_t'])\n",
    ")\n",
    "ws_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (ws_merged_df['div_str_id_s'].notna()) & (ws_merged_df['div_str_id'].isna())\n",
    "ws_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (ws_merged_df['div_str_id'].notna()) & (ws_merged_df['div_str_id_s'].isna())\n",
    "ws_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from ws_merged_df\n",
    "ws_merged_df = ws_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "ws_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = ws_merged_df[ws_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Weekly Sanitation Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Weekly_Sanitation_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Weekly Sanitation Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98eb6be",
   "metadata": {},
   "source": [
    "## 3) Weekly Rotation Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4399af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' \n",
    "and c.is_actv='Y' \n",
    "and DATE(strt_date) >='2023-09-03' \n",
    "and DATE(strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' \n",
    "and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' \n",
    "and c.is_actv='Y' \n",
    "and DATE(schd_strt_date) >='2023-09-03' \n",
    "and DATE(schd_strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' \n",
    "and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Rotation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date='2023-09-04' \n",
    "and task_status='To Do'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_wr_s_df = pd.merge(result_wr_s1_df, result_wr_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_wr_s_df = result_wr_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_wr_s_df = result_wr_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59682c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_wr_s_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_wr_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date='2023-09-05'\n",
    "and task_type='Rotation'\n",
    "and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_wr_t_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_wr_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff74649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "wr_merged_df = pd.merge(result_wr_s_df, result_wr_t_df, left_on=['div_str_id_s', 'task_desc']\n",
    "                        , right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "wr_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (wr_merged_df['task_type_name'] == wr_merged_df['task_type']) &\n",
    "    (wr_merged_df['task_classification_s'] == wr_merged_df['task_classification']) &\n",
    "    #(wr_merged_df['sub_dpmt_name'] == wr_merged_df['department_name']) &\n",
    "    (wr_merged_df['task_desc'] == wr_merged_df['task_description'])\n",
    "    #(wr_merged_df['concatenated_s'] == wr_merged_df['concatenated_t'])\n",
    ")\n",
    "wr_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (wr_merged_df['div_str_id_s'].notna()) & (wr_merged_df['div_str_id'].isna())\n",
    "wr_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (wr_merged_df['div_str_id'].notna()) & (wr_merged_df['div_str_id_s'].isna())\n",
    "wr_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from wr_merged_df\n",
    "wr_merged_df = wr_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "wr_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = wr_merged_df[wr_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Weekly Rotation Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Weekly_Rotation_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Weekly Rotation Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851517c",
   "metadata": {},
   "source": [
    "## 4) 3xWeekly Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT div_str_id, a.task_desc, a.task_type, task_type_name, task_classification, sub_dpmt_name\n",
    "            FROM santn_rotn_Task_mstr a\n",
    "            JOIN sub_dpmt_mpng b USING (sub_dpmt_id)\n",
    "            JOIN str_dpmt_mpng c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "            LEFT JOIN rolng_task_addtnl_info d ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE a.is_actv = 'Y' \n",
    "              AND c.is_actv = 'Y' \n",
    "              AND DATE(strt_date) = '2023-08-11' --and DATE(strt_date) <= '2023-08-11' \n",
    "              AND div_str_id = '53100515' \n",
    "              AND task_classification LIKE '3x Weekly'\n",
    "            UNION ALL\n",
    "            SELECT div_str_id, a.task_desc, a.task_type, task_type_name, task_classification, sub_dpmt_name\n",
    "            FROM santn_rotn_Task_mstr a\n",
    "            JOIN sub_dpmt_mpng b USING (sub_dpmt_id)\n",
    "            JOIN str_dpmt_mpng c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "            LEFT JOIN santn_rotn_task_schdl d ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE a.is_actv = 'Y' \n",
    "              AND c.is_actv = 'Y' \n",
    "              AND DATE(schd_strt_date) >= '2023-08-11' and DATE(schd_strt_date) <= '2023-08-11' \n",
    "              AND div_str_id = '53100515' \n",
    "              AND task_classification LIKE '3x Weekly'\n",
    "        ) \n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s1_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d00c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select distinct concat(division_number,store_number) AS div_str_id,\n",
    "from task where task_date='2023-08-11'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614706db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_3w_s_df = pd.merge(result_3w_s1_df, result_3w_s2_df,left_on=['div_str_id'], right_on=['div_str_id'], how='inner')\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_3w_s_df = result_3w_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_3w_s_df = result_3w_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e98083",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3w_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like '3x Weekly'\n",
    "and task_date='2023-08-11'\n",
    "and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_3w_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12caa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3w_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93570103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "w3_merged_df = pd.merge(result_3w_s_df, result_3w_t_df, left_on=['div_str_id_s', 'task_desc']\n",
    "                        , right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "w3_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (w3_merged_df['task_type_name'] == w3_merged_df['task_type']) &\n",
    "    (w3_merged_df['task_classification_s'] == w3_merged_df['task_classification']) &\n",
    "    #(w3_merged_df['sub_dpmt_name'] == w3_merged_df['department_name']) &\n",
    "    (w3_merged_df['task_desc'] == w3_merged_df['task_description'])\n",
    "    #(w3_merged_df['concatenated_s'] == w3_merged_df['concatenated_t'])\n",
    ")\n",
    "w3_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (w3_merged_df['div_str_id_s'].notna()) & (w3_merged_df['div_str_id'].isna())\n",
    "w3_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (w3_merged_df['div_str_id'].notna()) & (w3_merged_df['div_str_id_s'].isna())\n",
    "w3_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from w3_merged_df\n",
    "w3_merged_df = w3_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "w3_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = w3_merged_df[w3_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('3xWeekly Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'3xWeekly_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('3xWeekly Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a0ea4",
   "metadata": {},
   "source": [
    "## 5) Periodic Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(strt_date) >='2023-08-30' \n",
    "and DATE(strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Periodic' and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(schd_strt_date) >='2023-08-30' \n",
    "and DATE(schd_strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Periodic' and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s1_df = pd.DataFrame(rows, columns = cols)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Periodic' \n",
    "and task_date='2023-09-04' \n",
    "and task_status='To Do'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_s2_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9181c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_p_s_df = pd.merge(result_p_s1_df, result_p_s2_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_p_s_df = result_p_s_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_p_s_df = result_p_s_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cde8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_p_s_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_p_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c960f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id,\n",
    "task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Periodic'\n",
    "and task_date='2023-09-05'\n",
    "and concat(division_number, store_number) = '53100515'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_p_t_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_p_t_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_p_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ba152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "p_merged_df = pd.merge(result_p_s_df, result_p_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "p_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (p_merged_df['task_type_name'] == p_merged_df['task_type']) &\n",
    "    (p_merged_df['task_classification_s'] == p_merged_df['task_classification']) &\n",
    "    #(p_merged_df['sub_dpmt_name'] == p_merged_df['department_name']) &\n",
    "    (p_merged_df['task_desc'] == p_merged_df['task_description'])\n",
    "    #(p_merged_df['concatenated_s'] == p_merged_df['concatenated_t'])\n",
    ")\n",
    "p_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (p_merged_df['div_str_id_s'].notna()) & (p_merged_df['div_str_id'].isna())\n",
    "p_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (p_merged_df['div_str_id'].notna()) & (p_merged_df['div_str_id_s'].isna())\n",
    "p_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from p_merged_df\n",
    "p_merged_df = p_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "p_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = p_merged_df[p_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Periodic Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Periodic_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Periodic Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb86ef4",
   "metadata": {},
   "source": [
    "## 6) Quaterly Data Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_1 = \"udm-db\"  & instance_id = \"nc-event-sink-instance\" (Source Query)\n",
    "\n",
    "#Source Data\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT * FROM (\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and DATE(strt_date) >='2023-08-11' \n",
    "                and DATE(strt_date) <='2023-08-11' \n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "                AND div_str_id = '53100515'\n",
    "            UNION ALL\n",
    "            SELECT\n",
    "                div_str_id AS div_str_id_s,\n",
    "                task_type_name,\n",
    "                task_classification AS task_classification_s,\n",
    "                a.task_desc,\n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                and DATE(schd_strt_date) >='2023-08-11' \n",
    "                and DATE(schd_strt_date) <='2023-08-11'\n",
    "                AND task_classification LIKE 'Quarterly'\n",
    "                AND div_str_id = '53100515'\n",
    "        ) Daily_S\n",
    "    \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_s_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56630228",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_q_s_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_q_s_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb04f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#database_id_2 = \"storewalks-db\"  & instance_id = \"nc-event-sink-instance\"\n",
    "#Target Data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "        SELECT\n",
    "            CONCAT(division_number, store_number) AS div_str_id,\n",
    "            task_type,\n",
    "            task_classification,\n",
    "            task_description,\n",
    "        FROM\n",
    "            task\n",
    "        WHERE\n",
    "            task_type <> 'Green Rack'\n",
    "            AND task_classification LIKE 'Quarterly'\n",
    "            AND task_date = '2023-08-11'\n",
    "            AND CONCAT(division_number, store_number) = '53100515'\n",
    "    \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_q_t_df = pd.DataFrame(rows, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08e667",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows, num_columns = result_q_t_df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "result_q_t_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a942acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames on 'div_str_id' and 'task_desc'\n",
    "q_merged_df = pd.merge(result_q_s_df, result_q_t_df, left_on=['div_str_id_s', 'task_desc'], right_on=['div_str_id', 'task_description'], how='outer')\n",
    "\n",
    "# Create a new column 'ComparisonResult' based on your conditions\n",
    "q_merged_df['ComparisonResult'] = 'Mismatch'\n",
    "\n",
    "# Condition 1: Matched\n",
    "condition_matched = (\n",
    "    (q_merged_df['task_type_name'] == q_merged_df['task_type']) &\n",
    "    (q_merged_df['task_classification_s'] == q_merged_df['task_classification']) &\n",
    "    #(q_merged_df['sub_dpmt_name'] == q_merged_df['department_name']) &\n",
    "    (q_merged_df['task_desc'] == q_merged_df['task_description'])\n",
    "    #(q_merged_df['concatenated_s'] == q_merged_df['concatenated_t'])\n",
    ")\n",
    "q_merged_df.loc[condition_matched, 'ComparisonResult'] = 'Matched'\n",
    "\n",
    "# Condition 2: Record NOT in Target\n",
    "condition_not_in_target = (q_merged_df['div_str_id_s'].notna()) & (q_merged_df['div_str_id'].isna())\n",
    "q_merged_df.loc[condition_not_in_target, 'ComparisonResult'] = 'Record NOT in Target'\n",
    "\n",
    "# Condition 3: Record NOT in Source\n",
    "condition_not_in_source = (q_merged_df['div_str_id'].notna()) & (q_merged_df['div_str_id_s'].isna())\n",
    "q_merged_df.loc[condition_not_in_source, 'ComparisonResult'] = 'Record NOT in Source'\n",
    "\n",
    "# Remove duplicates from q_merged_df\n",
    "q_merged_df = q_merged_df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates (optional)\n",
    "q_merged_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter records where 'ComparisonResult' is not 'Matched'\n",
    "failed_records = q_merged_df[q_merged_df['ComparisonResult'] != 'Matched']\n",
    "\n",
    "# Check if there are any failed records\n",
    "if not failed_records.empty:\n",
    "    print('Quaterly Validation Report Failed')\n",
    "\n",
    "    # Get the current date as a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    # Define the full path to save the report file\n",
    "    file_path = r'C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports'\n",
    "    \n",
    "    # Define the file name with the date appended\n",
    "    report_file_name = f'Quaterly_Validation_Report_Failed_{timestamp}.xlsx'\n",
    "    \n",
    "    # Save the failed records to an Excel file with the generated file name\n",
    "    failed_records.to_excel(f'{file_path}\\\\{report_file_name}', index=False)\n",
    "else:\n",
    "    print('Quaterly Validation Report Passed')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5085e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c0eb5b",
   "metadata": {},
   "source": [
    "###  -------------- Counts  -------------- ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9199c2",
   "metadata": {},
   "source": [
    "## 1) Daily Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38b5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source Counts\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    " select sum(count), 'daily_s'  \n",
    "from    \n",
    " (\n",
    " (select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM rolng_task_addtnl_info) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                AND DATE(strt_date) = '2023-09-05'\n",
    "                AND DATE(strt_date) <= '2023-09-05'\n",
    "                AND task_classification LIKE 'Daily'\n",
    "group by div_str_id, task_type_name, task_classification,sub_dpmt_name)  \n",
    "union all\n",
    "(select  distinct count(*) as count,div_str_id, task_type_name, task_classification,sub_dpmt_name \n",
    "            FROM\n",
    "                (SELECT * FROM santn_rotn_task_mstr) a\n",
    "                JOIN (SELECT * FROM sub_dpmt_mpng) b USING (sub_dpmt_id)\n",
    "                JOIN (SELECT * FROM str_dpmt_mpng) c ON c.dpmt_name = b.prmry_dpmt_name\n",
    "                LEFT JOIN (SELECT * FROM santn_rotn_task_schdl) d\n",
    "                    ON (a.santn_rotn_task_mstr_id = d.santn_rotn_task_mstr_id)\n",
    "            WHERE\n",
    "                a.is_actv = 'Y'\n",
    "                AND c.is_actv = 'Y'\n",
    "                AND DATE(schd_strt_date) = '2023-09-05'\n",
    "                AND DATE(schd_strt_date) <= '2023-09-05'\n",
    "                AND task_classification LIKE 'Daily'\n",
    "group by div_str_id, task_type_name, task_classification,sub_dpmt_name) \n",
    ")   \n",
    "    \n",
    "        \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_s_c_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2c7a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Counts\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'daily_t' \n",
    "from \n",
    "(select count(*) as count1,division_number, store_number, task_type, task_classification,department_name, task_date, task_status \n",
    "from  task\n",
    "where task_type<>'Green Rack' and task_classification like 'Daily' and task_date='2023-09-05'\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status\n",
    ")\n",
    "        \"\"\")\n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_t_c_df = pd.DataFrame(rows, columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b215d",
   "metadata": {},
   "source": [
    "## 2) Weekly Sanitation Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75aefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(strt_date) >='2023-08-30' \n",
    "and DATE(strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(schd_strt_date) >='2023-08-30' \n",
    "and DATE(schd_strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s1_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3626fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Sanitation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date='2023-09-04' \n",
    "and task_status='To Do'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98b544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_ws_c_df = pd.merge(result_ws_s1_c_df, result_ws_s2_c_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_ws_c_df = result_ws_c_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_ws_c_df = result_ws_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_ws_c_count_df = result_ws_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count = result_ws_c_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label_df = pd.DataFrame({'count': [total_count], 'label': ['weekly_s']})\n",
    "\n",
    "#result_label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "397092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'weekly_t' \n",
    "from \n",
    "(select count(*) as count1, division_number,store_number,task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date='2023-09-05'\n",
    "and task_type='Sanitation'\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status,task_description\n",
    ")\n",
    "\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_ws_t_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc2beb",
   "metadata": {},
   "source": [
    "## 2) Weekly Rotation Counts Validation between source and target ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedd91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source data1\n",
    "with database_1.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select div_str_id,task_desc, task_type_name, task_classification,sub_dpmt_name from\n",
    "(\n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM rolng_task_addtnl_info)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id)  \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(strt_date) >='2023-09-03' \n",
    "and DATE(strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    "union all \n",
    "select div_str_id,a.task_desc,a.task_type, task_type_name, task_classification,sub_dpmt_name\n",
    "from (SELECT * FROM santn_rotn_Task_mstr) a \n",
    "join (SELECT * FROM sub_dpmt_mpng) b using(sub_dpmt_id) \n",
    "join (SELECT * FROM str_dpmt_mpng) c on c.dpmt_name=b.prmry_dpmt_name \n",
    "left join (SELECT * FROM santn_rotn_task_schdl)  d on (a.santn_rotn_task_mstr_id=d.santn_rotn_task_mstr_id) \n",
    "where a.is_actv='Y' and c.is_actv='Y' \n",
    "and DATE(schd_strt_date) >='2023-09-03' \n",
    "and DATE(schd_strt_date) <='2023-09-05' \n",
    "and task_classification  like 'Weekly' --and div_str_id ='53100515'\n",
    ") \n",
    "\n",
    " \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s1_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5498b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source data2\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select \n",
    "concat(division_number,store_number) AS div_str_id_2,\n",
    "task_type as task_type_2,\n",
    "task_classification as task_classification_2,\n",
    "department_name as department_name_2,\n",
    "task_description as task_description_2,\n",
    "from task\n",
    "where task_type='Rotation' \n",
    "and task_classification like 'Weekly' \n",
    "and task_date='2023-09-04' \n",
    "and task_status='To Do'\n",
    "        \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_s2_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b62c1400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>weekly_r_s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count       label\n",
       "0      0  weekly_r_s"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Joining the DataFrames using the specified conditions\n",
    "result_wr_c_df = pd.merge(result_wr_s1_c_df, result_wr_s2_c_df, how='inner', left_on=[\n",
    "    'div_str_id', 'task_type_name', 'task_classification','sub_dpmt_name', 'task_desc'\n",
    "], right_on=[\n",
    "    'div_str_id_2', 'task_type_2', 'task_classification_2', 'department_name_2', 'task_description_2'\n",
    "])\n",
    "\n",
    "# Select the desired columns in the result_df DataFrame\n",
    "result_wr_c_df = result_wr_c_df[['div_str_id', 'task_desc', 'task_type_name', 'task_classification', 'sub_dpmt_name']]\n",
    "\n",
    "# Rename the columns in the result_df DataFrame\n",
    "result_wr_c_df = result_wr_c_df.rename(columns={\n",
    "    'div_str_id': 'div_str_id_s',\n",
    "    'task_type_name': 'task_type_name',\n",
    "    'task_classification': 'task_classification_s',\n",
    "    'task_desc': 'task_desc',\n",
    "    'sub_dpmt_name': 'sub_dpmt_name'\n",
    "})\n",
    "\n",
    "# Group by 'div_str_id_s' and count the occurrences\n",
    "result_wr_c_count_df = result_wr_c_df.groupby('div_str_id_s').size().reset_index(name='count')\n",
    "\n",
    "# Calculate the sum of 'count'\n",
    "total_count2 = result_wr_c_count_df['count'].sum()\n",
    "\n",
    "# Create a DataFrame with 'count' and 'weekly_s' columns\n",
    "result_label2_df = pd.DataFrame({'count': [total_count2], 'label': ['weekly_r_s']})\n",
    "\n",
    "result_label2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd02e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target data\n",
    "with database_2.snapshot() as snapshot:\n",
    "    result = snapshot.execute_sql(\"\"\"\n",
    "select sum(count1), 'weekly_r_t' \n",
    "from \n",
    "(select count(*) as count1, division_number,store_number,task_type, task_classification, department_name, task_description\n",
    "from task\n",
    "where task_type<>'Green Rack' \n",
    "and task_classification  like 'Weekly'\n",
    "and task_date='2023-09-05'\n",
    "and task_type='Rotation'\n",
    "group by division_number, store_number, task_type, task_classification,department_name, task_date, task_status,task_description\n",
    ")\n",
    "\n",
    "    \"\"\")\n",
    "    \n",
    "    \n",
    "    # Stream in rows\n",
    "    rows = list()\n",
    "    for row in result:\n",
    "        rows.append(row)\n",
    "\n",
    "    # Get column names\n",
    "    cols = [x.name for x in result.fields]\n",
    "\n",
    "    # Convert to pandas dataframe\n",
    "    result_wr_t_c_df = pd.DataFrame(rows, columns = cols)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a58a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4972</td>\n",
       "      <td>weekly_r_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   \n",
       "0  4972  weekly_r_t"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_wr_t_c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def79757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b972a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfecc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f5a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7341a387",
   "metadata": {},
   "source": [
    "## Writing All counts to Report ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c7f195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current timestamp\n",
    "timestamp_c = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define the file path\n",
    "file_path_c = f\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\2023 Q3\\\\Store Walk-in\\\\Reports\\\\Count_{timestamp_c}.csv\"\n",
    "\n",
    "# Write the DataFrames to the CSV file\n",
    "result_s_c_df.to_csv(file_path_c, index=False) #daily\n",
    "result_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False)  #daily\n",
    "result_label_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly S\n",
    "result_ws_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly S\n",
    "result_label2_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly R\n",
    "result_wr_t_c_df.to_csv(file_path_c, mode='a', header=False, index=False) #weekly R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec1f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0738e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a0bd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a32e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec365245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac904ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22abb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf87bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee8c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29448730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ccee83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bd471f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d872c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401098f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc8b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a43d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b481485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counts_Report_Date \n",
    " Daily count Source - Done\n",
    " Daily count Target - Done\n",
    " Weekly Sanitation count Source - Done\n",
    " Weekly Sanitation count Target -Done\n",
    " Weekly Rotation count Source - Done \n",
    " Weekly Rotation count Target - Done\n",
    " 3xWeekly count Source - \n",
    " 3xWeekly count Target - \n",
    " Periodic count Source - \n",
    " Periodic count Target - \n",
    " \n",
    " ------------------------\n",
    "Daily_Validation_Report_Date - Done\n",
    "Weekly_Sanitation_Validation_Report_Date - Done\n",
    "Weekly_Rotation_Validation_Report_Date - Done\n",
    "3xWeekly_ReportValidation_Report_Date - DOne\n",
    "Periodic_Validation_Report_Date - Done\n",
    "Quaterly_Validation_Report_Date - Done\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_df.to_csv(\"C:\\\\Users\\\\AS81970\\\\Desktop\\\\tm.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
